# -*- coding: utf-8 -*-
"""SalesForcast.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rpzA_YlaAhKnOsYQlDIYMHbCE5Tf0YSR
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet, BayesianRidge, RANSACRegressor, HuberRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.decomposition import PCA, KernelPCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline

import warnings
warnings.filterwarnings('ignore')

"""#### *Exploratory Data analysis*"""

data = pd.read_csv('/content/kc_house_data.csv')
data.head()

data.shape

# see the datatype of each column
data.info()

# Checking any missing values
data.isnull().sum()

# fill the missing value with 0
data.fillna(0, inplace=True)

"""#### *Feature Creation*"""

# format the date (select the year from the value-"2014"1013T000000)
d = []
for i in data['date'].values:
    d.append(i[:4])  # selecting first 4 values from the date, that is the year
data['date'] = d

# Convert everything to same data type
for i in data.columns:
    data[i] = data[i].astype(float)

# make a new column for age of the house
data['age'] = data['date'] - data['yr_built']

# Calculate the total year of renovation
data['renov_age'] = np.abs(data['yr_renovated'] - data['yr_built'])

data['renov_age'] = data.renov_age.apply(lambda x: x if len(str(int(x)))==2 else 0.0) # if the renovated is less the 2 the value become 0 otherwise it subtracted from the year
                                                                                      # of renovated from yr_built.

# Remove unwanted columns like yr_built, id and date
data.drop(['id', 'date', 'yr_built', 'yr_renovated'], axis=1, inplace=True)
data.head()

"""#### *Dealing with highly correlated features*"""

data.corr()

# plotting correlation heatmap
plt.figure(figsize=(15,10))
dataplot = sns.heatmap(data.corr(), annot=True, cmap='Blues')
plt.show()

# print highly correlated variables
corr_features = []

for i,r in data.corr().iterrows():
    k=0
    for j in range(len(r)):
        if i!= r.index[k]:
            if r.values[k] >= 0.5:
                corr_features.append([i, r.index[k], r.values[k]])
        k += 1
corr_features

for i in corr_features:
    print(i)

# let us remove highly correlated features that is above 0.8
feat = []
for i in corr_features:
    if i[2] >= 0.8: # 2 is the correlation value as it is shown in the corr_feature output above.
        feat.append(i[0])
        feat.append(i[1])

data.drop(list(set(feat)), axis=1, inplace=True)

"""#### *Outlier Detection*

Price Feature Before Dealing With Outlier
"""

# creating boxplots to see the outliers in price variable

plt.figure(figsize=(6,4))
sns.boxplot(y=data['price'])
plt.show()

fig, ax = plt.subplots(ncols=7, nrows=3, figsize=(20,10))
index = 0
ax = ax.flatten()
for k,v in data.items():
    sns.boxplot(y=k, data=data, ax=ax[index])
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

# let us numerically draw conclusions
# creating function that can calculate inter quartile range of the data
def calc_interquartile(data, column):
    global lower, upper
    # calculating the first and third quartile
    first_quartile, third_quartile = np.percentile(data[column], 25), np.percentile(data[column], 75)
    # calculating the interquartile range
    iqr = third_quartile - first_quartile
    # outlier cutoff (1.5 is a generally taken as a threshold - Anything that goes below 1.5 and above 1.5 considered as a outlier.)
    cutoff = iqr*1.5
    # Calculate the lower and upper limit.
    lower, upper = first_quartile - cutoff, third_quartile + cutoff
    # Remove the outlier from the columns
    upper_outliers = data[data[column] > upper]
    lower_outliers = data[data[column] < lower]
    print('Lower outliers', lower_outliers.shape[0])
    print('Upper outliers', upper_outliers.shape[0])

    return print('Total outliers', upper_outliers.shape[0] + lower_outliers.shape[0])

# plotting outliers graph for 'price' feature
calc_interquartile(data, 'price')
plt.figure(figsize=(10,6))
sns.distplot(data['price'], kde=True)
print(upper, lower)
plt.axvspan(xmin=lower, xmax=data['price'].min(), alpha=0.2, color='blue', label='Lower Outliers')
plt.axvspan(xmin=upper, xmax=data['price'].max(), alpha=0.2, color='red', label='Upper Outliers')
plt.legend()
plt.show()

"""#### *Using ZScore*"""

""" creating function for calculating zscore which is subtracting the mean from every data point and dividing by the standard deviation and if the zscore value of any data point
is less than -3 or greater than 3, then that data point is an outlier"""

def z_score(data,column):
    # creating global variables for plotting the graph for better demonstration.
    global zscore, outlier
    # creating list to store zscore and outlier
    zscore = []
    outlier = []
    # for zscore generally taken thresholds are 2.5, 3 or 3.5, here i took 3
    threshold = 3
    # calculating the mean
    mean = np.mean(data[column])
    # calculating the standard deviation
    std = np.std(data[column])
    for i in data[column]:
        z = (i-mean)/std
        zscore.append(z)
        # if the zscore is greater than threshold = 3 that means it is an outlier.
        if np.abs(z) > threshold:
            outlier.append(i)
    return print('Total Outliers', len(outlier))

# plotting outliers graph for 'price' feature
z_score(data, 'price')
plt.figure(figsize=(10,6))
sns.distplot(zscore, kde=True)
print(upper, lower)
plt.axvspan(xmin= -3, xmax=min(zscore), alpha=0.2, color='blue', label='Lower Outliers')
plt.axvspan(xmin= 3, xmax=max(zscore), alpha=0.2, color='red', label='Upper Outliers')
plt.legend()
plt.show()

# Remove the outliers from the price column
dj = []
for i in data.price:
    if i in set(outlier):
        dj.append(0.0)
    else:
        dj.append(i)

data['P'] = dj
x = data.drop(data[data['P'] == 0.0].index)
x.shape

plt.figure(figsize=(10,5))
sns.distplot(x['price'], kde=True)
plt.show()

"""#### *Comparing The Data*"""

from sklearn.ensemble import IsolationForest

iso = IsolationForest()
outlier = iso.fit_predict(data)

outlier

print(set(outlier))

"""-1 for outlier and 1 for non-outlier."""

# mask variable contains all the outliers
mask = outlier == -1
# task variable contains all the non-outliers
task = outlier != -1
# creating dataframe containing outliers
df_1 = data[mask]
# creating dataframe containing non-outliers
df_2 = data[task]

# plotting graph to show the original data, outliers and non-outliers
plt.figure(figsize=(18,8))
plt.title('Plots of original data vs outliers from IsolationForest')
plt.hist(data['price'], label='Original', color='lightblue')
plt.hist(df_2['price'], label='Without outliers', color='green')
plt.hist(df_1['price'], label='outliers', color='red')
plt.legend()
plt.show()

"""#### *Model Building*"""

# defining the independent and dependent variables
X = x.drop(['price', 'P'], axis=1)
y = x['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
lr = LinearRegression()
lr.fit(X_train, y_train)
pred = lr.predict(X_test)
r2_score(y_test, pred)*100

"""#### **Creating Pipeline For All Models**"""

sc = ('Scaler', StandardScaler())
est = []
est.append(('GradientBoosting', Pipeline([sc, ('GradientBoosting', GradientBoostingRegressor())])))
est.append(('LinearRegression', Pipeline([sc, ('LinearRegression', LinearRegression())])))
est.append(('DecisionTree', Pipeline([sc, ('DecisionTree', DecisionTreeRegressor())])))
est.append(('RadomForest', Pipeline([sc, ('RandomForest', RandomForestRegressor())])))
est.append(('Kneighbors', Pipeline([sc, ('Kneighbors', KNeighborsRegressor())])))
est.append(('BayesianRidge', Pipeline([sc, ('BayesianRidge', BayesianRidge())])))
est.append(('ExtraTree', Pipeline([sc, ('ExtraTree', ExtraTreesRegressor())])))
est.append(('AdaBoost', Pipeline([sc, ('AdaBoost', AdaBoostRegressor())])))
est.append(('Bagging', Pipeline([sc, ('Bagging', BaggingRegressor())])))
est.append(('ElasticNet', Pipeline([sc, ('ElasticNet', ElasticNet())])))
est.append(('RANSAC', Pipeline([sc, ('RANSAC', RANSACRegressor())])))
est.append(('Huber', Pipeline([sc, ('Huber', HuberRegressor())])))
est.append(('XGB', Pipeline([sc, ('XGB', XGBRegressor())])))
est.append(('SGD', Pipeline([sc, ('SGD', SGDRegressor())])))
est.append(('Ridge', Pipeline([sc, ('Ridge', Ridge())])))
est.append(('Lasso', Pipeline([sc, ('Lasso', Lasso())])))

# using KFold cross-validation
seed = 4
splits = 7
score = 'r2'
model_score = []
for i in est:
    kfold = KFold(n_splits=splits, random_state=seed, shuffle=True)
    results = cross_val_score(i[1], X_train, y_train, cv=kfold, scoring=score)
    model_score.append({i[0] : '{}'.format(results.mean())})

model_score

"""#### **Hyperparameter Tuning**"""

# tuning only XGB because it has the higher accuracy.
est = []
est.append(('XGB', Pipeline([sc, ('XGB', XGBRegressor())])))

best = []
parameters = {
              'XGB' : {'XGB__learning_rate': [0.1, 0.2, 0.3, 0.4],
                       'XGB__max_depth': [4, 6, 8],
                       'XGB__n_estimators': [100, 500, 1000, 1500]}
}

for i in est:
    kfold = KFold(n_splits=5, random_state=seed, shuffle=True)
    grid = GridSearchCV(estimator=i[1], param_grid=parameters[i[0]], cv=kfold, n_jobs=-1)
    grid.fit(X_train, y_train)
    best.append((i[0], grid.best_score_, grid.best_params_))

best

# implementing with best parameters
xgb = XGBRegressor(learning_rate=0.1, max_depth=6, n_estimators=500)
xgb.fit(X_train, y_train)
pred = xgb.predict(X_test)
xgb.score(X_test, y_test)

X_train

"""### *Feature Selection*"""

from xgboost import plot_importance
plot_importance(xgb)
plt.show()

"""I will use the top 9 features as my final fetaures to build my model and deploy it. I can also consider all these important features shown above. NB: Lat and Long are very important features in this regard. They are telling us that the location of the building is an important factor in determining the price or value of the property. The zip code is also telling us the same thing. In this example, I will omit the Lat and Long because if i do the deployment and I want people to check the value of their property, it is mostly difficult for them to provide the exact latitude and logitute of their property. In reality, I would have gone extra mile to collect extra data on the location of the properties or houses and add that data to my dataset which i can use to build my model instead of using latitude and longitude.

I will rather include the zipcode which also tells us the location of the houses.
"""

x

Selected_features = x[['sqft_lot','sqft_living15','age','zipcode','bathrooms','bedrooms','renov_age','sqft_basement','grade']]
Selected_features

x_train, x_test, Y_train, y_test = train_test_split(Selected_features, y, test_size=0.3, random_state=42)

# implementing with best parameters
Xgb = XGBRegressor(learning_rate=0.1, max_depth=4, n_estimators=1000)
Xgb.fit(x_train, Y_train)
pred = Xgb.predict(x_test)
Xgb.score(x_test, y_test)

import pickle

pickle.dump(Xgb, open('model.pkl', 'wb'))